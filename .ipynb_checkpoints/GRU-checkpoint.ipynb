{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import os.path as op\n",
    "from csv import writer\n",
    "import math\n",
    "import cmath\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Model,Sequential,load_model\n",
    "from keras.layers import Input, Embedding\n",
    "from keras.layers import Dense, Bidirectional\n",
    "from keras.layers.recurrent import LSTM\n",
    "import keras.metrics as metrics\n",
    "import itertools\n",
    "from tensorflow.python.keras.utils.data_utils import Sequence\n",
    "from decimal import Decimal\n",
    "from keras.layers import Conv1D,MaxPooling1D,Flatten,Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.50000e+02 1.90401e+05 7.25000e+02 2.75500e+01 8.03900e+01]\n",
      " [1.50000e+02 1.90401e+05 8.25000e+02 2.75600e+01 8.03300e+01]\n",
      " [1.50000e+02 1.90401e+05 9.25000e+02 2.75800e+01 8.02400e+01]\n",
      " ...\n",
      " [6.10000e+01 1.91020e+05 1.94532e+05 2.93700e+01 7.52100e+01]\n",
      " [6.10000e+01 1.91020e+05 1.94632e+05 2.93500e+01 7.52700e+01]\n",
      " [6.10000e+01 1.91020e+05 1.94732e+05 2.93400e+01 7.53000e+01]]\n",
      "[[ 28.     3.   -52.   ...  16.97  19.63  20.06]\n",
      " [ 28.    15.   -53.   ...  16.63  19.57  23.06]\n",
      " [ 31.    16.   -55.   ...  17.24  19.98  20.24]\n",
      " ...\n",
      " [ 76.    12.   -76.   ...   3.47   3.95   4.35]\n",
      " [ 75.    13.   -76.   ...   3.88   4.33   4.42]\n",
      " [ 76.    12.   -75.   ...   3.46   4.07   4.28]]\n"
     ]
    }
   ],
   "source": [
    "A1=np.empty((0,5),dtype='float32')\n",
    "U1=np.empty((0,7),dtype='float32')\n",
    "node=['150','149','147','144','142','140','136','61']\n",
    "mon=['Apr','Mar','Aug','Jun','Jul','Sep','May','Oct']\n",
    "for j in node:\n",
    "  for i in mon:\n",
    "    inp= pd.read_csv('data_gkv/AT510_Node_'+str(j)+'_'+str(i)+'19_OutputFile.csv',usecols=[1,2,3,15,16])\n",
    "    out= pd.read_csv('data_gkv/AT510_Node_'+str(j)+'_'+str(i)+'19_OutputFile.csv',usecols=[5,6,7,8,17,18,19])\n",
    "    \n",
    "    inp=np.array(inp,dtype='float32')\n",
    "    out=np.array(out,dtype='float32')\n",
    "    \n",
    "    A1=np.append(A1, inp, axis=0)\n",
    "    U1=np.append(U1, out, axis=0)\n",
    "\n",
    "print(A1)\n",
    "print(U1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "scaler_obj=MinMaxScaler()\n",
    "X1=scaler_obj.fit_transform(A1)\n",
    "Y1=scaler_obj.fit_transform(U1)\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "\n",
    "X1=X1[:,np.newaxis,:]\n",
    "Y1=Y1[:,np.newaxis,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru (GRU)                    (None, 14)                882       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 14)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 7)                 105       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 7)                 28        \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 1,015\n",
      "Trainable params: 1,001\n",
      "Non-trainable params: 14\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(keras.Input(shape=(1,5)))\n",
    "model.add(tf.keras.layers.GRU(14,activation=\"relu\",use_bias=True,kernel_initializer=\"glorot_uniform\",bias_initializer=\"zeros\", \n",
    "                                kernel_regularizer=keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "                                bias_regularizer=keras.regularizers.l2(1e-4),\n",
    "                                activity_regularizer=keras.regularizers.l2(1e-5)))\n",
    "model.add(keras.layers.Dropout(.1))\n",
    "model.add(Dense(7))\n",
    "model.add(keras.layers.BatchNormalization(axis=-1,momentum=0.99,epsilon=0.001,center=True,scale=True,\n",
    "                                beta_initializer=\"zeros\",gamma_initializer=\"ones\",\n",
    "                                moving_mean_initializer=\"zeros\",moving_variance_initializer=\"ones\",trainable=True))\n",
    "model.add(keras.layers.ReLU())\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-5),loss='mse',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "761/761 [==============================] - 177s 231ms/step - loss: 0.2031 - accuracy: 0.0765 - val_loss: 0.3252 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/300\n",
      "761/761 [==============================] - 172s 226ms/step - loss: 0.1617 - accuracy: 0.0841 - val_loss: 0.2700 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/300\n",
      "761/761 [==============================] - 170s 223ms/step - loss: 0.1259 - accuracy: 0.0936 - val_loss: 0.1642 - val_accuracy: 2.4794e-06\n",
      "Epoch 4/300\n",
      "761/761 [==============================] - 170s 223ms/step - loss: 0.0961 - accuracy: 0.1222 - val_loss: 0.0946 - val_accuracy: 4.4493e-06\n",
      "Epoch 5/300\n",
      "761/761 [==============================] - 203s 267ms/step - loss: 0.0722 - accuracy: 0.1568 - val_loss: 0.0511 - val_accuracy: 5.3776e-06\n",
      "Epoch 6/300\n",
      "761/761 [==============================] - 197s 259ms/step - loss: 0.0530 - accuracy: 0.1577 - val_loss: 0.0272 - val_accuracy: 6.7135e-06\n",
      "Epoch 7/300\n",
      "761/761 [==============================] - 176s 231ms/step - loss: 0.0381 - accuracy: 0.1282 - val_loss: 0.0155 - val_accuracy: 8.4684e-06\n",
      "Epoch 8/300\n",
      "761/761 [==============================] - 172s 226ms/step - loss: 0.0267 - accuracy: 0.0687 - val_loss: 0.0094 - val_accuracy: 7.5740e-06\n",
      "Epoch 9/300\n",
      "761/761 [==============================] - 171s 225ms/step - loss: 0.0182 - accuracy: 0.0621 - val_loss: 0.0059 - val_accuracy: 6.7928e-06\n",
      "Epoch 10/300\n",
      "761/761 [==============================] - 169s 223ms/step - loss: 0.0120 - accuracy: 0.0603 - val_loss: 0.0038 - val_accuracy: 6.4701e-06\n",
      "Epoch 11/300\n",
      "761/761 [==============================] - 169s 223ms/step - loss: 0.0077 - accuracy: 0.0578 - val_loss: 0.0027 - val_accuracy: 6.6400e-06\n",
      "Epoch 12/300\n",
      "761/761 [==============================] - 171s 225ms/step - loss: 0.0049 - accuracy: 0.0574 - val_loss: 0.0021 - val_accuracy: 5.7626e-06\n",
      "Epoch 13/300\n",
      "761/761 [==============================] - 175s 230ms/step - loss: 0.0032 - accuracy: 0.0580 - val_loss: 0.0017 - val_accuracy: 5.0946e-07\n",
      "Epoch 14/300\n",
      "761/761 [==============================] - 175s 230ms/step - loss: 0.0023 - accuracy: 0.0538 - val_loss: 0.0015 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/300\n",
      "761/761 [==============================] - 181s 238ms/step - loss: 0.0019 - accuracy: 0.0500 - val_loss: 0.0014 - val_accuracy: 5.2644e-07\n",
      "Epoch 16/300\n",
      "761/761 [==============================] - 202s 266ms/step - loss: 0.0017 - accuracy: 0.0591 - val_loss: 0.0014 - val_accuracy: 9.7080e-06\n",
      "Epoch 17/300\n",
      "761/761 [==============================] - 179s 235ms/step - loss: 0.0016 - accuracy: 0.0583 - val_loss: 0.0013 - val_accuracy: 1.1593e-05\n",
      "Epoch 18/300\n",
      "761/761 [==============================] - 180s 236ms/step - loss: 0.0015 - accuracy: 0.0554 - val_loss: 0.0013 - val_accuracy: 1.0914e-05\n",
      "Epoch 19/300\n",
      "761/761 [==============================] - 173s 227ms/step - loss: 0.0015 - accuracy: 0.0622 - val_loss: 0.0013 - val_accuracy: 9.1137e-06\n",
      "Epoch 20/300\n",
      "761/761 [==============================] - 175s 230ms/step - loss: 0.0014 - accuracy: 0.0726 - val_loss: 0.0012 - val_accuracy: 8.5363e-06\n",
      "Epoch 21/300\n",
      "761/761 [==============================] - 166s 218ms/step - loss: 0.0013 - accuracy: 0.2984 - val_loss: 0.0013 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/300\n",
      "761/761 [==============================] - 188s 247ms/step - loss: 0.0011 - accuracy: 0.5029 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/300\n",
      "761/761 [==============================] - 175s 230ms/step - loss: 0.0010 - accuracy: 0.5833 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/300\n",
      "761/761 [==============================] - 170s 224ms/step - loss: 9.7712e-04 - accuracy: 0.6204 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/300\n",
      "761/761 [==============================] - 172s 226ms/step - loss: 9.2832e-04 - accuracy: 0.6265 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/300\n",
      "761/761 [==============================] - 171s 225ms/step - loss: 8.7020e-04 - accuracy: 0.7163 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/300\n",
      "761/761 [==============================] - 173s 227ms/step - loss: 8.0731e-04 - accuracy: 0.7641 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/300\n",
      "761/761 [==============================] - 176s 231ms/step - loss: 7.6122e-04 - accuracy: 0.7652 - val_loss: 9.9006e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/300\n",
      "761/761 [==============================] - 179s 235ms/step - loss: 7.2263e-04 - accuracy: 0.7656 - val_loss: 9.5981e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/300\n",
      "761/761 [==============================] - 172s 227ms/step - loss: 6.9128e-04 - accuracy: 0.7657 - val_loss: 9.3479e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/300\n",
      "761/761 [==============================] - 170s 224ms/step - loss: 6.6317e-04 - accuracy: 0.7651 - val_loss: 9.1128e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/300\n",
      "761/761 [==============================] - 171s 225ms/step - loss: 6.3719e-04 - accuracy: 0.7652 - val_loss: 8.8894e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/300\n",
      "761/761 [==============================] - 171s 225ms/step - loss: 6.1251e-04 - accuracy: 0.7665 - val_loss: 8.7349e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/300\n",
      "761/761 [==============================] - 180s 237ms/step - loss: 5.8122e-04 - accuracy: 0.7945 - val_loss: 8.3765e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/300\n",
      "761/761 [==============================] - 181s 238ms/step - loss: 5.3158e-04 - accuracy: 0.8358 - val_loss: 8.1187e-04 - val_accuracy: 1.1593e-05\n",
      "Epoch 36/300\n",
      "761/761 [==============================] - 172s 226ms/step - loss: 4.7323e-04 - accuracy: 0.8599 - val_loss: 7.9455e-04 - val_accuracy: 1.1593e-05\n",
      "Epoch 37/300\n",
      "761/761 [==============================] - 176s 232ms/step - loss: 4.5271e-04 - accuracy: 0.8600 - val_loss: 7.7898e-04 - val_accuracy: 1.1593e-05\n",
      "Epoch 38/300\n",
      "761/761 [==============================] - 177s 232ms/step - loss: 4.3720e-04 - accuracy: 0.8598 - val_loss: 7.6414e-04 - val_accuracy: 1.1593e-05\n",
      "Epoch 39/300\n",
      "761/761 [==============================] - 181s 238ms/step - loss: 4.2277e-04 - accuracy: 0.8599 - val_loss: 7.5037e-04 - val_accuracy: 1.1593e-05\n",
      "Epoch 40/300\n",
      "761/761 [==============================] - 182s 240ms/step - loss: 4.0936e-04 - accuracy: 0.8600 - val_loss: 7.3776e-04 - val_accuracy: 1.1593e-05\n",
      "Epoch 41/300\n",
      "761/761 [==============================] - 178s 234ms/step - loss: 3.9741e-04 - accuracy: 0.8599 - val_loss: 7.2617e-04 - val_accuracy: 1.1593e-05\n",
      "Epoch 42/300\n",
      "761/761 [==============================] - 178s 234ms/step - loss: 3.8618e-04 - accuracy: 0.8599 - val_loss: 7.1537e-04 - val_accuracy: 1.1593e-05\n",
      "Epoch 43/300\n",
      "761/761 [==============================] - 176s 231ms/step - loss: 3.7570e-04 - accuracy: 0.8600 - val_loss: 7.0515e-04 - val_accuracy: 1.1593e-05\n",
      "Epoch 44/300\n",
      "761/761 [==============================] - 176s 231ms/step - loss: 3.6599e-04 - accuracy: 0.8601 - val_loss: 6.9590e-04 - val_accuracy: 1.1593e-05\n",
      "Epoch 45/300\n",
      "761/761 [==============================] - 179s 235ms/step - loss: 3.5722e-04 - accuracy: 0.8599 - val_loss: 6.8735e-04 - val_accuracy: 1.1593e-05\n",
      "Epoch 46/300\n",
      "761/761 [==============================] - 178s 233ms/step - loss: 3.4916e-04 - accuracy: 0.8599 - val_loss: 6.7961e-04 - val_accuracy: 1.1593e-05\n",
      "Epoch 47/300\n",
      "761/761 [==============================] - 175s 230ms/step - loss: 3.4172e-04 - accuracy: 0.8600 - val_loss: 6.7262e-04 - val_accuracy: 1.1593e-05\n",
      "Epoch 48/300\n",
      "761/761 [==============================] - 173s 227ms/step - loss: 3.3509e-04 - accuracy: 0.8600 - val_loss: 6.6644e-04 - val_accuracy: 1.1593e-05\n",
      "Epoch 49/300\n",
      "761/761 [==============================] - 173s 227ms/step - loss: 3.2928e-04 - accuracy: 0.8599 - val_loss: 6.6089e-04 - val_accuracy: 1.1593e-05\n",
      "Epoch 50/300\n",
      "761/761 [==============================] - 173s 227ms/step - loss: 3.2403e-04 - accuracy: 0.8600 - val_loss: 6.5603e-04 - val_accuracy: 1.1593e-05\n",
      "Epoch 51/300\n",
      "761/761 [==============================] - 173s 228ms/step - loss: 3.1945e-04 - accuracy: 0.8600 - val_loss: 6.5169e-04 - val_accuracy: 1.1593e-05\n",
      "Epoch 52/300\n",
      "761/761 [==============================] - 165s 216ms/step - loss: 3.1535e-04 - accuracy: 0.8600 - val_loss: 6.4778e-04 - val_accuracy: 1.1593e-05\n",
      "Epoch 53/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175/761 [=====>........................] - ETA: 2:06 - loss: 3.1175e-04 - accuracy: 0.8593"
     ]
    }
   ],
   "source": [
    "history2 = model.fit(X1,Y1,batch_size=2048,epochs=300, validation_split=0.1)\n",
    "model_json = model.to_json()\n",
    "with open(\"gru.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model1.save_weights(\"gru.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history2.history['loss'])\n",
    "plt.plot(history2.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "plt.plot(history2.history['accuracy'])\n",
    "plt.plot(history2.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
